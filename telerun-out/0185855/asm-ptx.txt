
Fatbin elf code:
================
arch = sm_89
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin elf code:
================
arch = sm_89
code version = [1,7]
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_89
code version = [8,7]
host = linux
compile_size = 64bit
compressed
ptxasOptions = 

//
//
//
//
//
//

.version 8.7
.target sm_89
.address_size 64

//

.visible .entry _Z21mandelbrot_gpu_scalarjjPj(
.param .u32 _Z21mandelbrot_gpu_scalarjjPj_param_0,
.param .u32 _Z21mandelbrot_gpu_scalarjjPj_param_1,
.param .u64 _Z21mandelbrot_gpu_scalarjjPj_param_2
)
{
.reg .pred %p<14>;
.reg .f32 %f<27>;
.reg .b32 %r<12>;
.reg .b64 %rd<41>;


ld.param.u32 %r3, [_Z21mandelbrot_gpu_scalarjjPj_param_0];
ld.param.u32 %r4, [_Z21mandelbrot_gpu_scalarjjPj_param_1];
ld.param.u64 %rd22, [_Z21mandelbrot_gpu_scalarjjPj_param_2];
cvta.to.global.u64 %rd1, %rd22;
cvt.u64.u32 %rd2, %r3;
setp.eq.s32 %p1, %r3, 0;
@%p1 bra $L__BB0_17;

cvt.rn.f32.u32 %f1, %r3;
setp.eq.s32 %p2, %r4, 0;
@%p2 bra $L__BB0_8;

mov.u64 %rd23, 0;
mov.u64 %rd35, %rd23;

$L__BB0_3:
cvt.rn.f32.u64 %f10, %rd35;
div.approx.ftz.f32 %f11, %f10, %f1;
fma.rn.ftz.f32 %f2, %f11, 0f40200000, 0fBFA00000;
mul.lo.s64 %rd4, %rd35, %rd2;
mov.u64 %rd36, %rd23;

$L__BB0_4:
cvt.rn.f32.u64 %f15, %rd36;
div.approx.ftz.f32 %f16, %f15, %f1;
fma.rn.ftz.f32 %f3, %f16, 0f40200000, 0fC0000000;
mov.f32 %f24, 0f00000000;
mov.u32 %r11, 0;
mov.f32 %f25, %f24;
mov.f32 %f26, %f24;

$L__BB0_5:
mov.u32 %r1, %r11;
sub.ftz.f32 %f17, %f26, %f25;
add.ftz.f32 %f18, %f3, %f17;
sub.ftz.f32 %f19, %f24, %f26;
sub.ftz.f32 %f20, %f19, %f25;
add.ftz.f32 %f21, %f2, %f20;
mul.ftz.f32 %f26, %f18, %f18;
mul.ftz.f32 %f25, %f21, %f21;
add.ftz.f32 %f22, %f18, %f21;
mul.ftz.f32 %f24, %f22, %f22;
add.ftz.f32 %f23, %f26, %f25;
setp.le.ftz.f32 %p3, %f23, 0f40800000;
add.s32 %r11, %r1, 1;
setp.lt.u32 %p4, %r11, %r4;
and.pred %p5, %p3, %p4;
@%p5 bra $L__BB0_5;

add.s64 %rd25, %rd36, %rd4;
shl.b64 %rd26, %rd25, 2;
add.s64 %rd27, %rd1, %rd26;
add.s32 %r10, %r1, 1;
st.global.u32 [%rd27], %r10;
add.s64 %rd36, %rd36, 1;
setp.lt.u64 %p6, %rd36, %rd2;
@%p6 bra $L__BB0_4;

add.s64 %rd35, %rd35, 1;
setp.lt.u64 %p7, %rd35, %rd2;
@%p7 bra $L__BB0_3;
bra.uni $L__BB0_17;

$L__BB0_8:
add.s64 %rd8, %rd2, -1;
add.s64 %rd9, %rd1, 8;
and.b64 %rd10, %rd2, 3;
sub.s64 %rd11, %rd10, %rd2;
mov.u64 %rd28, 0;
mov.u64 %rd37, %rd28;

$L__BB0_9:
mul.lo.s64 %rd13, %rd37, %rd2;
setp.lt.u64 %p8, %rd8, 3;
mov.u64 %rd40, %rd28;
@%p8 bra $L__BB0_12;

shl.b64 %rd31, %rd13, 2;
add.s64 %rd38, %rd9, %rd31;
mov.u64 %rd40, 0;

$L__BB0_11:
mov.u32 %r6, 0;
st.global.u32 [%rd38+-8], %r6;
st.global.u32 [%rd38+-4], %r6;
st.global.u32 [%rd38], %r6;
st.global.u32 [%rd38+4], %r6;
add.s64 %rd38, %rd38, 16;
add.s64 %rd40, %rd40, 4;
add.s64 %rd32, %rd11, %rd40;
setp.ne.s64 %p9, %rd32, 0;
@%p9 bra $L__BB0_11;

$L__BB0_12:
setp.eq.s64 %p10, %rd10, 0;
@%p10 bra $L__BB0_16;

setp.eq.s64 %p11, %rd10, 1;
add.s64 %rd33, %rd40, %rd13;
shl.b64 %rd34, %rd33, 2;
add.s64 %rd20, %rd1, %rd34;
mov.u32 %r7, 0;
st.global.u32 [%rd20], %r7;
@%p11 bra $L__BB0_16;

setp.eq.s64 %p12, %rd10, 2;
st.global.u32 [%rd20+4], %r7;
@%p12 bra $L__BB0_16;

mov.u32 %r9, 0;
st.global.u32 [%rd20+8], %r9;

$L__BB0_16:
add.s64 %rd37, %rd37, 1;
setp.lt.u64 %p13, %rd37, %rd2;
@%p13 bra $L__BB0_9;

$L__BB0_17:
ret;

}
//
.visible .entry _Z21mandelbrot_gpu_vectorjjPj(
.param .u32 _Z21mandelbrot_gpu_vectorjjPj_param_0,
.param .u32 _Z21mandelbrot_gpu_vectorjjPj_param_1,
.param .u64 _Z21mandelbrot_gpu_vectorjjPj_param_2
)
{
.reg .pred %p<16>;
.reg .f32 %f<27>;
.reg .b32 %r<15>;
.reg .b64 %rd<44>;


ld.param.u32 %r3, [_Z21mandelbrot_gpu_vectorjjPj_param_0];
ld.param.u32 %r4, [_Z21mandelbrot_gpu_vectorjjPj_param_1];
ld.param.u64 %rd25, [_Z21mandelbrot_gpu_vectorjjPj_param_2];
cvta.to.global.u64 %rd1, %rd25;
cvt.u64.u32 %rd2, %r3;
setp.eq.s32 %p1, %r3, 0;
@%p1 bra $L__BB1_19;

mov.u32 %r5, %tid.x;
cvt.u64.u32 %rd3, %r5;
cvt.rn.f32.u32 %f1, %r3;
setp.eq.s32 %p2, %r4, 0;
@%p2 bra $L__BB1_9;

mov.u64 %rd38, 0;
cvt.u32.u64 %r6, %rd3;

$L__BB1_3:
setp.ge.u32 %p3, %r6, %r3;
@%p3 bra $L__BB1_8;

cvt.rn.f32.u64 %f10, %rd38;
div.approx.ftz.f32 %f11, %f10, %f1;
fma.rn.ftz.f32 %f2, %f11, 0f40200000, 0fBFA00000;
mul.lo.s64 %rd5, %rd38, %rd2;
mov.u64 %rd39, %rd3;

$L__BB1_5:
cvt.rn.f32.u64 %f15, %rd39;
div.approx.ftz.f32 %f16, %f15, %f1;
fma.rn.ftz.f32 %f3, %f16, 0f40200000, 0fC0000000;
mov.f32 %f24, 0f00000000;
mov.u32 %r14, 0;
mov.f32 %f25, %f24;
mov.f32 %f26, %f24;

$L__BB1_6:
mov.u32 %r1, %r14;
sub.ftz.f32 %f17, %f26, %f25;
add.ftz.f32 %f18, %f3, %f17;
sub.ftz.f32 %f19, %f24, %f26;
sub.ftz.f32 %f20, %f19, %f25;
add.ftz.f32 %f21, %f2, %f20;
mul.ftz.f32 %f26, %f18, %f18;
mul.ftz.f32 %f25, %f21, %f21;
add.ftz.f32 %f22, %f18, %f21;
mul.ftz.f32 %f24, %f22, %f22;
add.ftz.f32 %f23, %f26, %f25;
setp.le.ftz.f32 %p4, %f23, 0f40800000;
add.s32 %r14, %r1, 1;
setp.lt.u32 %p5, %r14, %r4;
and.pred %p6, %p4, %p5;
@%p6 bra $L__BB1_6;

add.s64 %rd27, %rd39, %rd5;
shl.b64 %rd28, %rd27, 2;
add.s64 %rd29, %rd1, %rd28;
add.s32 %r13, %r1, 1;
st.global.u32 [%rd29], %r13;
add.s64 %rd39, %rd39, 32;
setp.lt.u64 %p7, %rd39, %rd2;
@%p7 bra $L__BB1_5;

$L__BB1_8:
add.s64 %rd38, %rd38, 1;
setp.lt.u64 %p8, %rd38, %rd2;
@%p8 bra $L__BB1_3;
bra.uni $L__BB1_19;

$L__BB1_9:
not.b64 %rd31, %rd3;
add.s64 %rd9, %rd31, %rd2;
shr.u64 %rd32, %rd9, 5;
add.s64 %rd33, %rd32, 1;
and.b64 %rd10, %rd33, 3;
add.s64 %rd11, %rd3, 32;
add.s64 %rd12, %rd3, 64;
add.s64 %rd13, %rd3, 96;
add.s64 %rd14, %rd1, 256;
mov.u64 %rd40, 0;
cvt.u32.u64 %r8, %rd3;

$L__BB1_10:
setp.ge.u32 %p9, %r8, %r3;
@%p9 bra $L__BB1_18;

setp.eq.s64 %p10, %rd10, 0;
mul.lo.s64 %rd16, %rd40, %rd2;
mov.u64 %rd41, %rd3;
@%p10 bra $L__BB1_15;

setp.eq.s64 %p11, %rd10, 1;
add.s64 %rd34, %rd16, %rd3;
shl.b64 %rd35, %rd34, 2;
add.s64 %rd17, %rd1, %rd35;
mov.u32 %r9, 0;
st.global.u32 [%rd17], %r9;
mov.u64 %rd41, %rd11;
@%p11 bra $L__BB1_15;

setp.eq.s64 %p12, %rd10, 2;
st.global.u32 [%rd17+128], %r9;
mov.u64 %rd41, %rd12;
@%p12 bra $L__BB1_15;

mov.u32 %r11, 0;
st.global.u32 [%rd17+256], %r11;
mov.u64 %rd41, %rd13;

$L__BB1_15:
setp.lt.u64 %p13, %rd9, 96;
@%p13 bra $L__BB1_18;

add.s64 %rd36, %rd41, %rd16;
shl.b64 %rd37, %rd36, 2;
add.s64 %rd42, %rd14, %rd37;

$L__BB1_17:
mov.u32 %r12, 0;
st.global.u32 [%rd42+-256], %r12;
st.global.u32 [%rd42+-128], %r12;
st.global.u32 [%rd42], %r12;
st.global.u32 [%rd42+128], %r12;
add.s64 %rd42, %rd42, 512;
add.s64 %rd41, %rd41, 128;
setp.lt.u64 %p14, %rd41, %rd2;
@%p14 bra $L__BB1_17;

$L__BB1_18:
add.s64 %rd40, %rd40, 1;
setp.lt.u64 %p15, %rd40, %rd2;
@%p15 bra $L__BB1_10;

$L__BB1_19:
ret;

}


